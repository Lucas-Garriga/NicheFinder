{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29536b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Dossier : scraper\n",
      "📁 Dossier : Streamlit\n",
      "📄 Fichier : .DS_Store\n",
      "📄 Fichier : uv.lock\n",
      "📄 Fichier : pyproject.toml\n",
      "📄 Fichier : amazon_objets_connectes_v38.csv\n",
      "📁 Dossier : utils\n",
      "📄 Fichier : README.md\n",
      "📁 Dossier : .venv\n",
      "📄 Fichier : df_clean_2.csv\n",
      "📄 Fichier : .python-version\n",
      "📁 Dossier : .git\n",
      "📄 Fichier : main.py\n",
      "📁 Dossier : cleaner\n",
      "📁 Dossier : data\n",
      "📁 Dossier : notebooks\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Revenir au dossier parent (ici : NicheFinder)\n",
    "root = Path.cwd().parent\n",
    "\n",
    "# Affichage de tous les fichiers et dossiers de ce dossier principal\n",
    "for item in root.iterdir():\n",
    "    if item.is_dir():\n",
    "        print(f\"📁 Dossier : {item.name}\")\n",
    "    else:\n",
    "        print(f\"📄 Fichier : {item.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd95dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Script créé ici : /Users/samiraedoube/Downloads/CV/Data/NicheFinder/NicheFinder/run_etl.sh\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Chemin vers ton dossier principal (à adapter si besoin)\n",
    "chemin_principal = Path(\"/Users/samiraedoube/Downloads/CV/Data/NicheFinder/NicheFinder\")\n",
    "\n",
    "# Contenu du script bash pour lancer l'ETL\n",
    "script = \"\"\"#!/bin/bash\n",
    "# Script pour lancer l'ETL\n",
    "\n",
    "# Activation de l'environnement virtuel (à adapter si besoin)\n",
    "source ~/.venv/bin/activate\n",
    "\n",
    "# Lancement du script ETL\n",
    "python3 etl.py\n",
    "\"\"\"\n",
    "\n",
    "# Chemin complet vers le fichier à créer\n",
    "fichier_script = chemin_principal / \"run_etl.sh\"\n",
    "\n",
    "# Écriture du fichier\n",
    "with open(fichier_script, \"w\") as f:\n",
    "    f.write(script)\n",
    "\n",
    "print(f\"✅ Script créé ici : {fichier_script}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapping\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Configuration Selenium\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "base_url = \"https://www.amazon.fr/s?i=electronics&srs=4551203031&rh=n%3A4551203031&s=popularity-rank&fs=true&page={}\"\n",
    "\n",
    "products = []\n",
    "page = 1\n",
    "max_pages = 20  # Nombre de pages à scraper (ajuster si nécessaire)\n",
    "\n",
    "while page <= max_pages:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    driver.get(base_url.format(page))\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    items = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "    for rank, item in enumerate(items, start=1 + (page - 1) * len(items)):\n",
    "        title_elem = item.h2\n",
    "        title = title_elem.text.strip() if title_elem else None\n",
    "\n",
    "        price = None\n",
    "        price_whole = item.select_one(\"span.a-price > span.a-offscreen\")\n",
    "        if price_whole:\n",
    "            try:\n",
    "                price = float(\n",
    "                    price_whole.text.strip().replace(\"€\", \"\").replace(\",\", \".\")\n",
    "                )\n",
    "            except:\n",
    "                price = None\n",
    "\n",
    "        rating = None\n",
    "        rating_tag = item.select_one(\"span.a-icon-alt\")\n",
    "        if rating_tag:\n",
    "            rating_match = re.search(r\"(\\d+,\\d+)\", rating_tag.text)\n",
    "            if rating_match:\n",
    "                rating = float(rating_match.group(1).replace(\",\", \".\"))\n",
    "\n",
    "        # Votes (avis clients)\n",
    "        votes = None\n",
    "        votes_text = None\n",
    "        votes_elem = item.find(\"span\", class_=\"a-size-base s-underline-text\")\n",
    "        if votes_elem:\n",
    "            votes_text = votes_elem.text.strip()\n",
    "        else:\n",
    "            alt_votes = item.select_one(\"div.a-row.a-size-small span.a-size-base\")\n",
    "            if alt_votes:\n",
    "                votes_text = alt_votes.text.strip()\n",
    "        if votes_text:\n",
    "            cleaned = re.sub(r\"[^\\d]\", \"\", votes_text)\n",
    "            if cleaned.isdigit():\n",
    "                votes = int(cleaned)\n",
    "\n",
    "        # Ventes le mois dernier\n",
    "        sales = None\n",
    "        sales_elem = item.find(\"span\", class_=\"a-size-base a-color-secondary\")\n",
    "        if sales_elem and \"acheté\" in sales_elem.text:\n",
    "            sales_match = re.search(r\"(\\d[\\d\\s]+)\", sales_elem.text)\n",
    "            if sales_match:\n",
    "                sales = int(sales_match.group(1).replace(\" \", \"\").replace(\"\\u202f\", \"\"))\n",
    "\n",
    "        # Image\n",
    "        image_elem = item.find(\"img\")\n",
    "        image_url = image_elem[\"src\"] if image_elem else None\n",
    "\n",
    "        # Marque\n",
    "        brand = None\n",
    "        if title:\n",
    "            brand = title.split()[0]\n",
    "\n",
    "        # URL produit\n",
    "        url = None\n",
    "        link_tag = item.select_one(\"a.a-link-normal.s-link-style.a-text-normal\")\n",
    "        if link_tag and link_tag.get(\"href\"):\n",
    "            url = \"https://www.amazon.fr\" + link_tag[\"href\"]\n",
    "\n",
    "        # Prime (booléen)\n",
    "        prime = bool(item.select_one(\"i.a-icon-prime\"))\n",
    "\n",
    "        products.append(\n",
    "            {\n",
    "                \"title\": title,\n",
    "                \"brand\": brand,\n",
    "                \"price\": price,\n",
    "                \"rating\": rating,\n",
    "                \"votes\": votes,\n",
    "                \"sales_last_month\": sales,\n",
    "                \"image_url\": image_url,\n",
    "                \"url\": url,\n",
    "                \"prime\": prime,\n",
    "                \"category\": \"Objets connectés\",\n",
    "                \"rank\": rank,\n",
    "                \"scraped_at\": datetime.now(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    page += 1\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Sauvegarde\n",
    "dfv38 = pd.DataFrame(products)\n",
    "dfv38.to_csv(\"amazon_objets_connectes_v38.csv\", index=False)\n",
    "print(\"✅ Données sauvegardées : amazon_objets_connectes_v38.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_data\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(__file__).resolve().parent.parent  # remonte à la racine\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from utils.io_utils import get_project_root, make_dir\n",
    "\n",
    "import pandas as pd\n",
    "from utils.io_utils import get_project_root, make_dir\n",
    "\n",
    "\n",
    "def clean_amazon() -> Path:\n",
    "    \"\"\"Lit data/raw_data.csv, nettoie, écrit data/df_clean.csv, retourne son Path.\"\"\"\n",
    "    root = get_project_root()\n",
    "    data_dir = root / \"data\"\n",
    "    raw_path = data_dir / \"raw_data.csv\"\n",
    "    clean_path = data_dir / \"df_clean.csv\"\n",
    "\n",
    "    if not raw_path.exists():\n",
    "        raise FileNotFoundError(f\"{raw_path} introuvable. Lance d’abord le scraping.\")\n",
    "\n",
    "    df = pd.read_csv(raw_path)\n",
    "\n",
    "    # Nettoyage rapide\n",
    "    df = df.drop_duplicates().dropna(subset=[\"title\"])\n",
    "    num_cols = [\"price\", \"rating\", \"votes\"]\n",
    "    for col in num_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    df.to_csv(clean_path, index=False)\n",
    "    print(f\"✅ Données nettoyées : {clean_path}\")\n",
    "    return clean_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
